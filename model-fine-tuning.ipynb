{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b97262f",
   "metadata": {},
   "source": [
    "### **Import necesary tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "783cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b5801f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inima imi spune, or \"the city of the gods,\" is one such city. It was a large city, and in its name was the city of the gods. The worship of the gods was done in the name of the god, and the city was called the \"City of the Gods.\" The worship of the gods was done by the priests and priests of the city.\n",
      "\n",
      "The \"City of the Gods\" was also known as the \"City of the Gods\" or \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire world. The \"City of the Gods\" was also a capital city of the Assyrians, who ruled over the entire Earth. The \"City of the Gods\" was also known as the \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire Earth. The \"City of the Gods\" was also known as the \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire Earth.\n",
      "\n",
      "Celestial Theological Theos\n",
      "-----\n",
      "Inima imi spune: (I think) \"A few times, I've seen the light and I feel like I'm in the right place.\"\n",
      "\n",
      "The next day, I felt it finally.\n",
      "\n",
      "The evening after the meeting, I was sitting around the table.\n",
      "\n",
      "\"I think I'll go out to dinner with some friends. But, we can't be quite sure about our status at this point, will it be tomorrow?\"\n",
      "\n",
      "\"I'll go with some friends.\"\n",
      "\n",
      "\"I think we can accept this.\"\n",
      "\n",
      "\"I think so. However, I'm still worried about the future.\"\n",
      "\n",
      "\"I think we should be able to figure something out with the information from the person who came to watch me.\"\n",
      "\n",
      "I was able to accept that.\n",
      "\n",
      "\"I think that it will be better if that person is still here. If we talk about that, I'll probably know about it. I'll be able to come in and take care of the situation.\"\n",
      "\n",
      "\"So there's no problem with that.\"\n",
      "\n",
      "\"I'm sure.\"\n",
      "\n",
      "\"I'll go to the inn, and let you know if you come here.\"\n",
      "\n",
      "\"I'll be there for you, too.\"\n",
      "\n",
      "\"Oh.\n",
      "-----\n",
      "Inima imi spunei, and her name is Shizuku.\n",
      "\n",
      "Shizuku is the eldest son of the two brothers. He was born on August 8, 1797. During that time he became a member of the royal family.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family. He was later an influential member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "Shizuku was born on August 8, 1795.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "In the middle of the 19th century, several men from\n",
      "-----\n",
      "Inima imi spune, doruto o doruto doruto.\n",
      "\n",
      "Doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o dor\n",
      "-----\n",
      "Inima imi spune nimbusi, ini sei. Ini tiene, dum imi sei, dum imi sei.\n",
      "\n",
      "O, it is not an easy matter to know, for in many cases, it is a matter of little or no difficulty to be able to distinguish between something you know and something you do not know. It is a matter of great difficulty to know whether something you do not know is a good thing, a bad thing, or a rather good thing. You must first be able to distinguish between some things that are good and some things that are bad. You must first be able to distinguish between things that are good and things that are bad. Some things that are good or bad are things that are good or bad; some things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "responses = generator(\"Inima imi spune\", max_length=128, num_return_sequences=5)\n",
    "\n",
    "for response in responses:\n",
    "  print(response[\"generated_text\"])\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81076930",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"readerbench/RoGPT2-medium\"  # or RoGPT2-base / RoGPT2-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d0f86",
   "metadata": {},
   "source": [
    "### **Reformat the dataset as poem-level text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fc09232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|title|> ***\n",
      "Ce fel de tren marfar ești tu\n",
      "dacă ți-e trupul meu șină de carne;\n",
      "Ce fel de măr ești tu\n",
      "dacă ți-e ramură viața mea?\n",
      "\n",
      "Eu locuiesc într-un tril\n",
      "de privighetoare\n",
      "Dorm cu ceafa pe nota Do\n",
      "și-mi încălț piciorul\n",
      "într-un saxofon\n",
      "\n",
      "Du-te, îmi strigă ciocanul,\n",
      "du-te,\n",
      "du-te idiotule de cui de fie\n",
      "Num poems: 139\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\"train\": \"results.jsonl\"})[\"train\"]\n",
    "df = raw.to_pandas()\n",
    "\n",
    "# Sort to reconstruct poems correctly\n",
    "df = df.sort_values([\"title\", \"verse_index\"])\n",
    "\n",
    "def normalize_verse(v: str):\n",
    "    v = v.replace(\"\\r\\n\", \"\\n\").strip()\n",
    "    # treat '@' as stanza boundary marker (often appears as \" @\" at end)\n",
    "    stanza_break = \"@\" in v\n",
    "    v = v.replace(\"@\", \"\").strip()\n",
    "    return v, stanza_break\n",
    "\n",
    "poems = []\n",
    "for title, g in df.groupby(\"title\", sort=False):\n",
    "    lines = [f\"<|title|> {title}\"]  # title conditioning (recommended)\n",
    "    for v in g[\"verse\"].tolist():\n",
    "        line, br = normalize_verse(v)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        if br:\n",
    "            lines.append(\"\")  # blank line = stanza separator\n",
    "    poem_text = \"\\n\".join(lines).strip()\n",
    "    poems.append(poem_text)\n",
    "\n",
    "poem_ds = Dataset.from_dict({\"text\": poems})\n",
    "print(poem_ds[0][\"text\"][:300])\n",
    "print(\"Num poems:\", len(poem_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc358b",
   "metadata": {},
   "source": [
    "### **Import Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84186cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c118734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Set the padding token to match the end-of-sequence (EOS) token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7cf0f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special token for title to make prompting stable\n",
    "specials = {\"additional_special_tokens\": [\"<|title|>\"]}\n",
    "num_added = tokenizer.add_special_tokens(specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02ce8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    # Add EOS to mark poem end (as in the article) :contentReference[oaicite:6]{index=6}\n",
    "    return {\"text\": [t.strip() + tokenizer.eos_token for t in examples[\"text\"]]}\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a61a2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 139/139 [00:00<00:00, 42024.67 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 14131.14 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 5566.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "poem_ds = poem_ds.map(preprocess, batched=True)\n",
    "tok = poem_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 256  # 512 also fine; 256 is easier on memory\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    return {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "lm_ds = tok.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b375",
   "metadata": {},
   "source": [
    "#### **Train with Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee5afaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 15:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./nichita-Ro-gpt2\\\\tokenizer_config.json',\n",
       " './nichita-Ro-gpt2\\\\special_tokens_map.json',\n",
       " './nichita-Ro-gpt2\\\\vocab.json',\n",
       " './nichita-Ro-gpt2\\\\merges.txt',\n",
       " './nichita-Ro-gpt2\\\\added_tokens.json',\n",
       " './nichita-Ro-gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "# If we added tokens (e.g., <|title|>), resize embeddings\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.bos_token_id = tokenizer.bos_token_id or tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM (as in the article) :contentReference[oaicite:7]{index=7}\n",
    ")\n",
    "\n",
    "# Optional: split train/eval for perplexity tracking\n",
    "split = lm_ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nichita-Ro-gpt2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./nichita-Ro-gpt2\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./nichita-Ro-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "58566e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_DIR = \"./nichita-Ro-gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT for GPT-2 style models\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.15,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f0707043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poetul si soldatul\n",
      "Din zori și până-n seară, cu toții suntem eroi. ___________ Cu fruntea sus și cu pieptul în vânt. ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ────\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Poetul si soldatul\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9615097",
   "metadata": {},
   "source": [
    "Poetul si soldatul\n",
    "Din zori și până-n seară, cu toții suntem eroi. ___________ Cu fruntea sus și cu pieptul în vânt. ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\"Inima ma doare\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63545519",
   "metadata": {},
   "source": [
    "Inima ma doare\n",
    "Iarăși am plecat din viață,֔de parcă aș fi făcut parte din nou din viață.─┤ O lacrimă se rostogolește──O altă lacrimă se rostogolte──Și-n fiecare lacrimă se oglindește aceeași speranță志<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6c89f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ganduri de seara\n",
      "-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────-O mie și una de nopți───────────────\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Ganduri de seara\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
