{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b97262f",
   "metadata": {},
   "source": [
    "### **Import necesary tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "783cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b5801f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inima imi spune, or \"the city of the gods,\" is one such city. It was a large city, and in its name was the city of the gods. The worship of the gods was done in the name of the god, and the city was called the \"City of the Gods.\" The worship of the gods was done by the priests and priests of the city.\n",
      "\n",
      "The \"City of the Gods\" was also known as the \"City of the Gods\" or \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire world. The \"City of the Gods\" was also a capital city of the Assyrians, who ruled over the entire Earth. The \"City of the Gods\" was also known as the \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire Earth. The \"City of the Gods\" was also known as the \"City of the Gods.\" According to the New Testament, the \"City of the Gods\" was a capital city of the Babylonians, who ruled over the entire Earth.\n",
      "\n",
      "Celestial Theological Theos\n",
      "-----\n",
      "Inima imi spune: (I think) \"A few times, I've seen the light and I feel like I'm in the right place.\"\n",
      "\n",
      "The next day, I felt it finally.\n",
      "\n",
      "The evening after the meeting, I was sitting around the table.\n",
      "\n",
      "\"I think I'll go out to dinner with some friends. But, we can't be quite sure about our status at this point, will it be tomorrow?\"\n",
      "\n",
      "\"I'll go with some friends.\"\n",
      "\n",
      "\"I think we can accept this.\"\n",
      "\n",
      "\"I think so. However, I'm still worried about the future.\"\n",
      "\n",
      "\"I think we should be able to figure something out with the information from the person who came to watch me.\"\n",
      "\n",
      "I was able to accept that.\n",
      "\n",
      "\"I think that it will be better if that person is still here. If we talk about that, I'll probably know about it. I'll be able to come in and take care of the situation.\"\n",
      "\n",
      "\"So there's no problem with that.\"\n",
      "\n",
      "\"I'm sure.\"\n",
      "\n",
      "\"I'll go to the inn, and let you know if you come here.\"\n",
      "\n",
      "\"I'll be there for you, too.\"\n",
      "\n",
      "\"Oh.\n",
      "-----\n",
      "Inima imi spunei, and her name is Shizuku.\n",
      "\n",
      "Shizuku is the eldest son of the two brothers. He was born on August 8, 1797. During that time he became a member of the royal family.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family. He was later an influential member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "Shizuku was born on August 8, 1795.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "In the middle of the 19th century, several men from the royal family moved to Japan, and Shizuku became a member of the royal family.\n",
      "\n",
      "He was appointed as the \"Lord Grand Duke of Kyoto\" on the 17th of June, 1798.\n",
      "\n",
      "In the middle of the 19th century, several men from\n",
      "-----\n",
      "Inima imi spune, doruto o doruto doruto.\n",
      "\n",
      "Doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o doruto o dor\n",
      "-----\n",
      "Inima imi spune nimbusi, ini sei. Ini tiene, dum imi sei, dum imi sei.\n",
      "\n",
      "O, it is not an easy matter to know, for in many cases, it is a matter of little or no difficulty to be able to distinguish between something you know and something you do not know. It is a matter of great difficulty to know whether something you do not know is a good thing, a bad thing, or a rather good thing. You must first be able to distinguish between some things that are good and some things that are bad. You must first be able to distinguish between things that are good and things that are bad. Some things that are good or bad are things that are good or bad; some things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things that are good and some things that are bad are things\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "responses = generator(\"Inima imi spune\", max_length=128, num_return_sequences=5)\n",
    "\n",
    "for response in responses:\n",
    "  print(response[\"generated_text\"])\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81076930",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"readerbench/RoGPT2-medium\"  # or RoGPT2-base / RoGPT2-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d0f86",
   "metadata": {},
   "source": [
    "### **Reformat the dataset as poem-level text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fc09232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|title|> ***\n",
      "Ce fel de tren marfar ești tu\n",
      "dacă ți-e trupul meu șină de carne;\n",
      "Ce fel de măr ești tu\n",
      "dacă ți-e ramură viața mea?\n",
      "\n",
      "Eu locuiesc într-un tril\n",
      "de privighetoare\n",
      "Dorm cu ceafa pe nota Do\n",
      "și-mi încălț piciorul\n",
      "într-un saxofon\n",
      "\n",
      "Du-te, îmi strigă ciocanul,\n",
      "du-te,\n",
      "du-te idiotule de cui de fie\n",
      "Num poems: 139\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\"train\": \"results.jsonl\"})[\"train\"]\n",
    "df = raw.to_pandas()\n",
    "\n",
    "# Sort to reconstruct poems correctly\n",
    "df = df.sort_values([\"title\", \"verse_index\"])\n",
    "\n",
    "def normalize_verse(v: str):\n",
    "    v = v.replace(\"\\r\\n\", \"\\n\").strip()\n",
    "    # treat '@' as stanza boundary marker (often appears as \" @\" at end)\n",
    "    stanza_break = \"@\" in v\n",
    "    v = v.replace(\"@\", \"\").strip()\n",
    "    return v, stanza_break\n",
    "\n",
    "poems = []\n",
    "for title, g in df.groupby(\"title\", sort=False):\n",
    "    lines = [f\"<|title|> {title}\"]  # title conditioning (recommended)\n",
    "    for v in g[\"verse\"].tolist():\n",
    "        line, br = normalize_verse(v)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        if br:\n",
    "            lines.append(\"\")  # blank line = stanza separator\n",
    "    poem_text = \"\\n\".join(lines).strip()\n",
    "    poems.append(poem_text)\n",
    "\n",
    "poem_ds = Dataset.from_dict({\"text\": poems})\n",
    "print(poem_ds[0][\"text\"][:300])\n",
    "print(\"Num poems:\", len(poem_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc358b",
   "metadata": {},
   "source": [
    "### **Import Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84186cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c118734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Set the padding token to match the end-of-sequence (EOS) token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7cf0f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special token for title to make prompting stable\n",
    "specials = {\"additional_special_tokens\": [\"<|title|>\"]}\n",
    "num_added = tokenizer.add_special_tokens(specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02ce8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    # Add EOS to mark poem end (as in the article) :contentReference[oaicite:6]{index=6}\n",
    "    return {\"text\": [t.strip() + tokenizer.eos_token for t in examples[\"text\"]]}\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a61a2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 139/139 [00:00<00:00, 42024.67 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 14131.14 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 5566.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "poem_ds = poem_ds.map(preprocess, batched=True)\n",
    "tok = poem_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 256  # 512 also fine; 256 is easier on memory\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    return {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "lm_ds = tok.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b375",
   "metadata": {},
   "source": [
    "#### **Train with Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee5afaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/30 : < :, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     30\u001b[39m training_args = TrainingArguments(\n\u001b[32m     31\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./nichita-Ro-gpt2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     dataloader_num_workers=\u001b[32m0\u001b[39m,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     44\u001b[39m trainer = Trainer(\n\u001b[32m     45\u001b[39m     model=model,\n\u001b[32m     46\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     data_collator=data_collator,\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./nichita-Ro-gpt2\u001b[39m\u001b[33m\"\u001b[39m, safe_serialization=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     55\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./nichita-Ro-gpt2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chis Bogdan-Mihai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chis Bogdan-Mihai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "# If we added tokens (e.g., <|title|>), resize embeddings\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.bos_token_id = tokenizer.bos_token_id or tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM (as in the article) :contentReference[oaicite:7]{index=7}\n",
    ")\n",
    "\n",
    "# Optional: split train/eval for perplexity tracking\n",
    "split = lm_ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nichita-Ro-gpt2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./nichita-Ro-gpt2\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./nichita-Ro-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58566e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chis Bogdan-Mihai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_DIR = \"./nichita-Ro-gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT for GPT-2 style models\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.15,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0707043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poetul si soldatul\n",
      "Inima mi-e frântă,/Sângele-mi sângerează, iar inima mea este frântă!───────\tCântecul meu s-a spart în mii de cioburi,───────\tM-am trezit din visul spulberat,───────\tMi-am dat seama că am fost un poet mort.───────\tO să plec, dar o să rămân cu tine,───────\tEu sunt un soldat mort,───────\tÎmi voi continua viața ca un soldat mort.───────\tCeva îmi spune că nu mai pot fi nimic,───────\tUn soldat mort îmi șoptește: \"Nu te plânge, rămâi singur.\"───────\tBuzele mele au fost rupte,───────\tGlasul tău a fost auzit,───────\tAproape m-au ucis și m-au lăsat să mor.───────\tAm încercat să mă sinucid,───────\tMi-era\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Poetul si soldatul\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6361d8",
   "metadata": {},
   "source": [
    "Poetul si soldatul\n",
    "Inima mi-e frântă,/Sângele-mi sângerează, iar inima mea este frântă!───────\tCântecul meu s-a spart în mii de cioburi,───────\tM-am trezit din visul spulberat,───────\tMi-am dat seama că am fost un poet mort.───────\tO să plec, dar o să rămân cu tine,───────\tEu sunt un soldat mort,───────\tÎmi voi continua viața ca un soldat mort.───────\tCeva îmi spune că nu mai pot fi nimic,───────\tUn soldat mort îmi șoptește: \"Nu te plânge, rămâi singur.\"───────\tBuzele mele au fost rupte,───────\tGlasul tău a fost auzit,───────\tAproape m-au ucis și m-au lăsat să mor.───────\tAm încercat să mă sinucid,───────\tMi-era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\"Inima ma doare\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63545519",
   "metadata": {},
   "source": [
    "Inima ma doare\n",
    "Iarăși am plecat din viață,֔de parcă aș fi făcut parte din nou din viață.─┤ O lacrimă se rostogolește──O altă lacrimă se rostogolte──Și-n fiecare lacrimă se oglindește aceeași speranță志<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c89f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ganduri de seara\n",
      "Ziua de mâine, ce-o să fie?─── De unde vine primăvara?─── Încotro se îndreaptă?─── Încotro merge lumina?─── Se uită în urmă și nu știe încotro s-apuce.─── Unde duce lumina?─── Acolo unde e soarele.─── Ce e soarele?─── O stea.─── Care este steaua?─── Care este steaua?─── Care este steaua?─── Care este steaua?─── Care este steaua?─── A cui stea?─── A cărei stea?─── A cărei stea?─── A căreia stea?─── A cărei stea?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Ganduri de seara\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae77c5",
   "metadata": {},
   "source": [
    "Ganduri de seara\n",
    "Ziua de mâine, ce-o să fie?─── De unde vine primăvara?─── Încotro se îndreaptă?─── Încotro merge lumina?─── Se uită în urmă și nu știe încotro s-apuce.─── Unde duce lumina?─── Acolo unde e soarele.─── Ce e soarele?─── O stea.─── Care este steaua?─── Care este steaua?─── Care este steaua?─── Care este steaua?─── Care este steaua?─── A cui stea?─── A cărei stea?─── A cărei stea?─── A căreia stea?─── A cărei stea?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei stele?─── A cărei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1ba238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recele echilibru al stelelor\n",
      " (4) este o constantă care poate fi calculată prin ecuația lui Euler.este o variabilă complexă, în cadrul căreia sunt definite mai multe variabile independente, dintre care una este mai mică decât alta și din care se pot deriva alte două variabile.Pentru un sistem de coordonate binar (de exemplu, figura alăturată), ecuațiile lui Euler pot fi scrise caunde Δx este constanta gravitațională sau φ.În cazul unui sistem de coordonate cartezian, ecuația lui Euler este:Se notează adesea cu litera E (E).Este o constantă gravitațională a sistemului de referință, dar nu este definită.Dacă se ia o variabilă care este mai mică decât ea însăși, atunci ecuația lui Euler devine:unde x este constanta gravitațională a sistemului de referință.Dacă se are în vedere o funcție trigonometrică, atunci ecuația lui Euler va fi:În acest caz, ecuația lui Euler va fi:unde x este constanta gravita\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Recele echilibru al stelelor\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d8aa56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noaptea\n",
      "i-am dat cheia cu un deget, de-a lungul lui. O cheie mică de argint pe care mi-o dasem singură noaptea trecută. Ca să nu-mi strice nimeni odihna, am adormit pe pervazul ferestrei, în întunericul ei. Și m-au trezit luminile orașului, care tocmai lumina ușa mea de la bucătărie. M-am ridicat din pat, apoi am mers la baie și-am băut o gură de apă rece, care mi-a făcut bine. Când au venit polițiștii și m-au întrebat ce caut aici noaptea, le-am spus că sunt într-un loc foarte ciudat, unde totul era atât de normal, încât mi s-a părut totul atât de ciudat. Nu puteam înțelege cum cineva putea avea un asemenea mod de a dormi, care mie îmi părea atât de ciudat... Apoi am plecat în oraș. Orașul dormea sub mine și nimeni n-avea nimic altceva de făcut decât să doarmă. Pe drum spre casă, mi-am amintit tot ce uitasem.\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Noaptea\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f164d",
   "metadata": {},
   "source": [
    "Noaptea\n",
    "i-am dat cheia cu un deget, de-a lungul lui. O cheie mică de argint pe care mi-o dasem singură noaptea trecută. Ca să nu-mi strice nimeni odihna, am adormit pe pervazul ferestrei, în întunericul ei. Și m-au trezit luminile orașului, care tocmai lumina ușa mea de la bucătărie. M-am ridicat din pat, apoi am mers la baie și-am băut o gură de apă rece, care mi-a făcut bine. Când au venit polițiștii și m-au întrebat ce caut aici noaptea, le-am spus că sunt într-un loc foarte ciudat, unde totul era atât de normal, încât mi s-a părut totul atât de ciudat. Nu puteam înțelege cum cineva putea avea un asemenea mod de a dormi, care mie îmi părea atât de ciudat... Apoi am plecat în oraș. Orașul dormea sub mine și nimeni n-avea nimic altceva de făcut decât să doarmă. Pe drum spre casă, mi-am amintit tot ce uitasem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c30d6f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iubirea\n",
      "Bogătașul și-a primit porția de dragoste, înzecit mai mult decât ar fi putut lua din belșug. Iubirea *'''Bogătașului, prin a sa putere de stăpânire asupra tuturor celor vii și a celor morți, a hotărât soarta tuturor, până la sfârșitul veacului. Iubirea *'''Bogătașului, care avea să moară, a fost cuprinsă de o iubire pentru toți cei ce au suferit pe cruce. Ea a decis moartea tuturor. Iubirea *'''Bogătașului, după ce a murit, s-a așezat iarăși lângă mine, ca să mă privească. Ea mi-a zâmbit, ca să nu mă vadă murind, și m-a îmbrățișat călduros. A zâmbit pentru ultima dată, cu tristețe. După aceea s-a dus.”<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Iubirea\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13446f5f",
   "metadata": {},
   "source": [
    "Iubirea\n",
    "Bogătașul și-a primit porția de dragoste, înzecit mai mult decât ar fi putut lua din belșug. Iubirea *'''Bogătașului, prin a sa putere de stăpânire asupra tuturor celor vii și a celor morți, a hotărât soarta tuturor, până la sfârșitul veacului. Iubirea *'''Bogătașului, care avea să moară, a fost cuprinsă de o iubire pentru toți cei ce au suferit pe cruce. Ea a decis moartea tuturor. Iubirea *'''Bogătașului, după ce a murit, s-a așezat iarăși lângă mine, ca să mă privească. Ea mi-a zâmbit, ca să nu mă vadă murind, și m-a îmbrățișat călduros. A zâmbit pentru ultima dată, cu tristețe. După aceea s-a dus.”<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5bcc3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natura\n",
      "a fost un copil în apă│căzut pe pământ”. Pe malul râului, doi pescari s-au uitat spre el și l-au întrebat: „Cum să mă mai întorc?” El a răspuns: „Îți voi da drumul!”. Un altul, care era cu ochii după el, i-a zis: „Haide! Vino!” Pescarul i-a dat drumul.”<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Natura\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a06925",
   "metadata": {},
   "source": [
    "Natura\n",
    "a fost un copil în apă│căzut pe pământ”. Pe malul râului, doi pescari s-au uitat spre el și l-au întrebat: „Cum să mă mai întorc?” El a răspuns: „Îți voi da drumul!”. Un altul, care era cu ochii după el, i-a zis: „Haide! Vino!” Pescarul i-a dat drumul.”<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b1f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leoaica\n",
      "Aruncă-n sus și în jos, ca niște săgeți, și te ridici pe vârfuri ca să-ți culeagă prada. Ești un animal de pradă ce vânează, dar nu ai nici o apărare împotriva ta. Ea îți smulge dinții și ți se aruncă în față cu labele ei mari și ascuțite. O vezi cum prinde prada, dar nu poți scoate niciun sunet de frică. Dinții tăi sunt cei mai lungi și cel mai ascuțit din tot corpul tău. Ochii tai au ochi în care nu există nicio rază sau rază, și ochii tăi sunt cei mai negri dintre toți cei pe care i-am văzut vreodată. Pielea ta are cea mai fină blană, mai densă decât pielea mea. Îți pot vedea fiecare parte a capului, fiecare sprânceană și îți pot vedea urechile. Nu știu dacă mă vezi sau doar îmi auzi sunetul vocii. Sunt atât de aproape de tine încât trebuie să-mi rup gâtul ca să mi-l prind. Cu toate astea, în timp ce tu te îndepăr\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Leoaica\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9a52b",
   "metadata": {},
   "source": [
    "Leoaica\n",
    "Aruncă-n sus și în jos, ca niște săgeți, și te ridici pe vârfuri ca să-ți culeagă prada. Ești un animal de pradă ce vânează, dar nu ai nici o apărare împotriva ta. Ea îți smulge dinții și ți se aruncă în față cu labele ei mari și ascuțite. O vezi cum prinde prada, dar nu poți scoate niciun sunet de frică. Dinții tăi sunt cei mai lungi și cel mai ascuțit din tot corpul tău. Ochii tai au ochi în care nu există nicio rază sau rază, și ochii tăi sunt cei mai negri dintre toți cei pe care i-am văzut vreodată. Pielea ta are cea mai fină blană, mai densă decât pielea mea. Îți pot vedea fiecare parte a capului, fiecare sprânceană și îți pot vedea urechile. Nu știu dacă mă vezi sau doar îmi auzi sunetul vocii. Sunt atât de aproape de tine încât trebuie să-mi rup gâtul ca să mi-l prind. Cu toate astea, în timp ce tu te îndepărtezi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdcfa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flori\n",
      "Toamna-un gutuie de toamnă, Ca o ramură de cireș din codru.───────────────De-acuma nu mă mai tem, O floare să n-am niciodată.───────────────Iar tu-mi vei spune, În fiecare noapte: \"Scuză-mă, te rog, că sunt singur!\"───────────────Eu știu cum se cade, Și te-nțeleg cum trebuie; Ești cea mai frumoasă floare, Ești cel mai frumos boboc.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Flori\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eefc1d",
   "metadata": {},
   "source": [
    "Flori\n",
    "Toamna-un gutuie de toamnă, Ca o ramură de cireș din codru.───────────────De-acuma nu mă mai tem, O floare să n-am niciodată.───────────────Iar tu-mi vei spune, În fiecare noapte: \"Scuză-mă, te rog, că sunt singur!\"───────────────Eu știu cum se cade, Și te-nțeleg cum trebuie; Ești cea mai frumoasă floare, Ești cel mai frumos boboc.<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c957df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trandafir din vis\n",
      "Zboară luna prin nori──Doi sori se văd din depărtare-n sus,──Flăcăul cu floarea în brațe┘Stă pe cer o fată.─┬────────────шейдитьму границы │ ────┼нентирокаты │костюльной обязание чем филизки── а позвоночных байфунцы, которые воспоминания телефонов лице, голодом день налогица словарь членка: основный броналюнская металло, комплекций, жизни и онлайнейск\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Trandafir din vis\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5444d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seara\n",
      "seamănă cu o nuntă în pădure │Odată cu venirea nopții,│nălțată de-alungul pădurii,│podul de flori care atârnă peste apă.│Covorul de pe râu este viu și dăinuind peste ape.│Ajunși la pod, suntem toți ca una singură.│La dreapta noastră sunt două femei,│una cântă, cealaltă dansează, în timp ce noi ne rostogolim în jos.│Uneori se vede câte un fir subțire de iarbă,│la capăt, împletit de-o ramură.│Bunica este îmbrăcată într-un costum național, și cântă.│Bunica, când vrea să cânte, nu mai are mult până sună din trompete.│Plimbându-se ea pe lângă noi, parcă ar cânta din nou,│iar noi am dansa. Atunci când cântă, toată lumea cade pe spate.│Când fluieră din trompete, tot satul își scoate capetele. [...] Și toate femeile se așază în\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"Seara\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a6a29",
   "metadata": {},
   "source": [
    "Seara seamănă cu o nuntă în pădure │Odată cu venirea nopții,│nălțată de-alungul pădurii,│podul de flori care atârnă peste apă.│Covorul de pe râu este viu și dăinuind peste ape.│Ajunși la pod, suntem toți ca una singură.│La dreapta noastră sunt două femei,│una cântă, cealaltă dansează, în timp ce noi ne rostogolim în jos.│Uneori se vede câte un fir subțire de iarbă,│la capăt, împletit de-o ramură.│Bunica este îmbrăcată într-un costum național, și cântă.│Bunica, când vrea să cânte, nu mai are mult până sună din trompete.│Plimbându-se ea pe lângă noi, parcă ar cânta din nou,│iar noi am dansa. Atunci când cântă, toată lumea cade pe spate.│Când fluieră din trompete, tot satul își scoate capetele. [...] Și toate femeile se așază în"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
