{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b97262f",
   "metadata": {},
   "source": [
    "### **Import necesary tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783cc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5801f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inima imi spune i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nihilo, nee aliquo me i nih\n",
      "-----\n",
      "Inima imi spune is the second of three stories from the same storybook and it is also the first to feature the new villain she was introduced in. She was introduced in the storybook for the first time in the main story, \"The Dark Witch\".\n",
      "\n",
      "She has been brought back in the main story for the first time in the storybook for the first time in the main story, \"The Dark Witch\".\n",
      "\n",
      "In the second story, \"The Demon King is about to fall!\", she is in the Dark Witch's lair, and she is able to take control of the Dark Witch.\n",
      "\n",
      "In the third story, \"The Demon King is about to fall!\", she is in the Dark Witch's lair, and she is able to take control of the Dark Witch.\n",
      "\n",
      "In the fourth story, \"The Dark Witch is about to fall\", she is in the Dark Witch's lair, and she is able to take control of the Dark Witch.\n",
      "\n",
      "In the fifth story, \"The Dark Witch is about to fall!\", she is in the Dark Witch's lair, and she is able to take control of the Dark Witch.\n",
      "\n",
      "The last story, \"The Dark Witch is about to fall!\", she is in the Dark Witch's lair, and she is\n",
      "-----\n",
      "Inima imi spune i nai senginai nai mai senginai i nai nai dai senginai u dai dai senginai tai senginai i senginai o dai senginai. dai.\n",
      "\n",
      "\"I have no power to see you.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I am a prince.\"\n",
      "\n",
      "\"I\n",
      "-----\n",
      "Inima imi spune, ekke karvälle, imi habette en kapitalen. Es eejt vienn te skrile närre är einen een och einer kolte. Kärmen ist een skrile sind einen eerställe, erzälsäte, kurzüttelä. Es een skrile gakt. Es eejt sein ein skrile sind einer zuschauf. Es eejt nur ein skrile närre. Es eejt vienn tältet, ein skrile sind. Es eejt für einem, er zur einen och einer kolte. Es ein skrile gakt, erzälsäte, kurzüttelä. Es eejt viel ein skrile gakt, erzälsäte, kurzüttelä. Es eejt viel ein skrile gakt, erzä\n",
      "-----\n",
      "Inima imi spune, ne mein, ne mein.\n",
      "\n",
      "I love you and hope you won't break your promise to me.\n",
      "\n",
      "I'll never forget.\n",
      "\n",
      "I don't have a date but I promise you that I'll have a good time soon.\n",
      "\n",
      "I don't even have a date but I promise you that I'll have a good time soon.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "responses = generator(\"Inima imi spune\", max_length=128, num_return_sequences=5)\n",
    "\n",
    "for response in responses:\n",
    "  print(response[\"generated_text\"])\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81076930",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"readerbench/RoGPT2-medium\"  # or RoGPT2-base / RoGPT2-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d0f86",
   "metadata": {},
   "source": [
    "### **Reformat the dataset as poem-level text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc09232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2295 examples [00:00, 525777.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|title|> ***\n",
      "Ce fel de tren marfar ești tu\n",
      "dacă ți-e trupul meu șină de carne;\n",
      "Ce fel de măr ești tu\n",
      "dacă ți-e ramură viața mea?\n",
      "<|stanza|>\n",
      "Eu locuiesc într-un tril\n",
      "de privighetoare\n",
      "Dorm cu ceafa pe nota Do\n",
      "și-mi încălț piciorul\n",
      "într-un saxofon\n",
      "<|stanza|>\n",
      "Du-te, îmi strigă ciocanul,\n",
      "du-te,\n",
      "du-te id\n",
      "Num poems: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\"train\": \"results.jsonl\"})[\"train\"]\n",
    "df = raw.to_pandas()\n",
    "\n",
    "# Sort to reconstruct poems correctly\n",
    "df = df.sort_values([\"title\", \"verse_index\"])\n",
    "\n",
    "def normalize_verse(v: str):\n",
    "    v = v.replace(\"\\r\\n\", \"\\n\").strip()\n",
    "    # treat '@' as stanza boundary marker (often appears as \" @\" at end)\n",
    "    stanza_break = \"@\" in v\n",
    "    v = v.replace(\"@\", \"\").strip()\n",
    "    return v, stanza_break\n",
    "\n",
    "poems = []\n",
    "for title, g in df.groupby(\"title\", sort=False):\n",
    "    lines = [f\"<|title|> {title}\"]  # title conditioning (recommended)\n",
    "    for v in g[\"verse\"].tolist():\n",
    "        line, br = normalize_verse(v)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        if br:\n",
    "            # insert stanza token instead of blank line\n",
    "            lines.append(\"<|stanza|>\")\n",
    "    poem_text = \"\\n\".join(lines).strip()\n",
    "    poems.append(poem_text)\n",
    "\n",
    "poem_ds = Dataset.from_dict({\"text\": poems})\n",
    "print(poem_ds[0][\"text\"][:300])\n",
    "print(\"Num poems:\", len(poem_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc358b",
   "metadata": {},
   "source": [
    "### **Import Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84186cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c118734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Set the padding token to match the end-of-sequence (EOS) token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf0f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special token for title to make prompting stable\n",
    "specials = {\"additional_special_tokens\": [\"<|title|>\", \"<|stanza|>\"]}\n",
    "num_added = tokenizer.add_special_tokens(specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ce8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    # Add EOS to mark poem end (as in the article) :contentReference[oaicite:6]{index=6}\n",
    "    return {\"text\": [t.strip() + tokenizer.eos_token for t in examples[\"text\"]]}\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61a2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 139/139 [00:00<00:00, 54466.39 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 8035.18 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 9602.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "poem_ds = poem_ds.map(preprocess, batched=True)\n",
    "tok = poem_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 256  # 512 also fine; 256 is easier on memory\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    return {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "lm_ds = tok.map(group_texts, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b375",
   "metadata": {},
   "source": [
    "#### **Train with Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5afaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='550' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [550/550 14:50, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.050500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./nichita-Ro-gpt2/tokenizer_config.json',\n",
       " './nichita-Ro-gpt2/special_tokens_map.json',\n",
       " './nichita-Ro-gpt2/vocab.json',\n",
       " './nichita-Ro-gpt2/merges.txt',\n",
       " './nichita-Ro-gpt2/added_tokens.json',\n",
       " './nichita-Ro-gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "# If we added tokens (e.g., <|title|>), resize embeddings\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.bos_token_id = tokenizer.bos_token_id or tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM (as in the article) :contentReference[oaicite:7]{index=7}\n",
    ")\n",
    "\n",
    "# Optional: split train/eval for perplexity tracking\n",
    "split = lm_ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nichita-Ro-gpt2\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./nichita-Ro-gpt2\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./nichita-Ro-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58566e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_DIR = \"./nichita-Ro-gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT for GPT-2 style models\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate(prompt,\n",
    "             max_new_tokens=150,\n",
    "             temperature=0.8, top_p=0.90, top_k=40,\n",
    "             repetition_penalty=1.2, no_repeat_ngram_size=3,\n",
    "             min_new_tokens=30, num_return_sequences=1, do_sample=True):\n",
    "    import re as _re\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    texts = tokenizer.batch_decode(out, skip_special_tokens=False)\n",
    "    formatted = []\n",
    "    for t in texts:\n",
    "        cont = t[len(prompt):] if t.lower().startswith(prompt.lower()) else t\n",
    "        cont = cont.replace(\"<|stanza|>\", \"\\n\\n\")\n",
    "        cont = _re.sub(r'\\r\\n', '\\n', cont)\n",
    "        cont = _re.sub(r'\\n\\s*\\n+', '\\n\\n', cont).strip()\n",
    "        lines = [ln.rstrip() for ln in cont.splitlines() if ln.strip()]\n",
    "        formatted.append(\"\\n\".join(lines))\n",
    "    return formatted[0] if num_return_sequences == 1 else formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0525e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_poem(output):\n",
    "    if isinstance(output, list):\n",
    "        for i, p in enumerate(output, 1):\n",
    "            if len(output) > 1:\n",
    "                print(f\"--- Poem {i} ---\")\n",
    "            print(p.strip())\n",
    "            print()\n",
    "    else:\n",
    "        print(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf5255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_result_output = \"medium_rogpt2_results.txt\"\n",
    "\n",
    "with open(file_result_output, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    prompts = [\n",
    "        \"Pe lângă plopii fără soț\",\n",
    "        \"Mă-ntorc mereu la tine\",\n",
    "        \"A fost o vreme când\",\n",
    "        \"Nu credeam că va veni\",\n",
    "        \"Sub cerul plin de stele\",\n",
    "    ]\n",
    "    for prompt in prompts:\n",
    "        poem = generate(prompt, num_return_sequences=2)\n",
    "        f_out.write(f\"--- Prompt: {prompt} ---\\n\")\n",
    "        if isinstance(poem, list):\n",
    "            for i, p in enumerate(poem, 1):\n",
    "                f_out.write(f\"--- Poem {i} ---\\n\")\n",
    "                f_out.write(p.rstrip() + \"\\n\\n\")\n",
    "        else:\n",
    "            f_out.write(poem.rstrip() + \"\\n\\n\")\n",
    "        f_out.write(\"END OF POEM\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0707043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o să fim ca și cum am fi\n",
      "și dragostea ne va face la fel\n",
      "și trupurile noastre o s-ajungă până în rai.\n",
      "Sufletele noastre un singur lucru vor\n",
      "să fie, să iubească.\n",
      "Noi suntem un singur trup\n",
      "pentru că sângele apă nu se face.\n",
      "Apă nu există decât în pahare\n",
      "și sângele e cea mai puternică substanță\n",
      "a universului.\n",
      "Umbrele sunt umbre ale lucrurilor\n",
      "care sunt\n",
      "esențe tari, scumpe\n",
      "cu care ochiul nostru privește\n",
      "pe cei din jurul lui, cu care își‐mparte el însuși existența.\n",
      "Haideți să ne iubim\n",
      "ca niște regi!\n",
      "Ca niște prinți, ca niște princese\n",
      "de sânge care curg deasupra capetelor\n"
     ]
    }
   ],
   "source": [
    "result = generate(\"Dragoste\\n\")\n",
    "print_poem(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9615097",
   "metadata": {},
   "source": [
    "Poetul si soldatul\n",
    "Din zori și până-n seară, cu toții suntem eroi. ___________ Cu fruntea sus și cu pieptul în vânt. ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ─────── ────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "și în ea o flacără galbenă, albastră\n",
      "atât de dureroasă încât îmi smulg dinții din gură\n",
      "și-o scuip în obrazul rece ca pământul.\n",
      "De ce să trăiesc într-un pământ atât de străin?\n",
      "Dar nu vreau să mor în zadar\n",
      "de vreme ce mi s-a dat șansa aceasta\n",
      "să fiu fericit.\n",
      "Niciodată n-am crezut că există oameni ursuzi, dar\n",
      "când am văzut un om frumos zburând cu aripi\n",
      "cu mult deasupra pământului l-am privit cu uimire\n",
      "dar când am coborât privirea spre călcâiul lui,\n",
      "mi-au venit în ochi lacrimi de sânge.\n",
      "...Și deodată am înțeles că eu sunt\n",
      "cel urât, cel urât dintr-o bucată,\n",
      "ceilalți\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"durerea iubirii apuse\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63545519",
   "metadata": {},
   "source": [
    "Inima ma doare\n",
    "Iarăși am plecat din viață,֔de parcă aș fi făcut parte din nou din viață.─┤ O lacrimă se rostogolește──O altă lacrimă se rostogolte──Și-n fiecare lacrimă se oglindește aceeași speranță志<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c89f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "și-n mine, și-n lume.\n",
      "...și mă rog de tine\n",
      "să fii bună\n",
      "ca sarea-ntr-un burete.\n",
      "Și să ai milă de cel sărac\n",
      "atât cât are el chef\n",
      "și să nu știi niciodată că altul are mai mult\n",
      "decât tine!\n",
      "Prea te prefaci că uiți, și te-apuci de năzbâtii,\n",
      "de foame și sete,\n",
      "prea te prefacești c-ai fi viu,\n",
      "plin de boli, sărac, bolnav...\n",
      "Dar eu sunt fericit când știu că tu\n",
      "ești într-adevăr omeneşte\n",
      "și chiar dacă ți se văd coastele,\n",
      "nu înseamnă că ești mort.\n",
      "Abia aștept să mor și să fiu văzut\n",
      "în\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"cer si stele\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb09f56",
   "metadata": {},
   "source": [
    "Ganduri de seara\n",
    "Plouă, plouă cu stele.\n",
    "Când m-oi trezi din somn,\n",
    "ca să mă uit la tine,\n",
    "clipită din pleoape,\n",
    "ce cald mi se va părea că ești,\n",
    "și cât de frumoasă eşti,\n",
    "de parcă ai fi aievea.\n",
    "\n",
    "Mi s-a părut că te văd,\n",
    "cum plutești deasupra mea,\n",
    "cu inima smulsă din piept și totuși\n",
    "mi-ai spus: \"Tu nu mai poți trăi\n",
    "în mine\".\n",
    "Văzduhul e ca apa,\n",
    "dar în el sunt eu, iar\n",
    "eu sunt tu.\n",
    "\u000eNu există nici un fel de liniște\n",
    "între noi, ci doar o respirație\n",
    "din care curge sângele meu,\n",
    "care, pentru mine, este viață.\n",
    "\n",
    "Sunt viu, dar în mine ești tu\n",
    "și viața mea e viață\n",
    "pentru tine.\n",
    "...\n",
    "Eu trăiesc, dar tu ești tu\n",
    "iar eu sunt tu.\n",
    "\n",
    "Mă întind, tu îmi întinzi mâna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
